{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c07ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  # This imports the openai package.\n",
    "import nltk  # This imports the nltk package.\n",
    "import re  # This imports the re package.\n",
    "\n",
    "# The above packages are third-party dependencies that will need to be installed.\n",
    "# They can be listed in a requirements.txt file to make it easier to manage and\n",
    "# install project dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4306c464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\varun\\anaconda3\\lib\\site-packages (0.27.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\varun\\anaconda3\\lib\\site-packages (from openai) (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\varun\\anaconda3\\lib\\site-packages (from openai) (4.64.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: attrs>=17.3.0 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from aiohttp->openai) (5.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\varun\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\varun\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.5.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed9d6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.9.12\n",
      "re:  2.2.1\n",
      "nltk:  3.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\varun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import platform  # Module to check the current Python version\n",
    "import os  # Module to interact with the operating system\n",
    "import docx2txt  # A library to extract text from Word documents\n",
    "import openai  # The OpenAI API client library\n",
    "import re  # Module for regular expressions\n",
    "from os.path import splitext, exists  # Functions for file paths and existence checks\n",
    "import nltk  # The Natural Language Toolkit\n",
    "nltk.download('punkt')  # Download the required Punkt tokenizer data\n",
    "from nltk.tokenize import word_tokenize  # Tokenization function from NLTK\n",
    "import datetime  # Module for working with dates and times\n",
    "\n",
    "# Print Python version and package versions for debugging purposes\n",
    "print('Python: ', platform.python_version())\n",
    "print('re: ', re.__version__)\n",
    "print('nltk: ', nltk.__version__)\n",
    "\n",
    "# Method for cleaning up the content of a docx file\n",
    "def clean_docx(file_path: str) -> str:\n",
    "    \"\"\"Clean up the content of a docx file to a string\n",
    "\n",
    "    Args:\n",
    "        file_path (str): path to docx file\n",
    "\n",
    "    Returns:\n",
    "        str: clean content\n",
    "    \"\"\"\n",
    "    # Read the content of the file\n",
    "    content = docx2txt.process(file_path)\n",
    "\n",
    "    # Remove timestamps from the content\n",
    "    pattern = r\"\\d+:\\d+:\\d+.\\d+\\s-->\\s\\d+:\\d+:\\d+.\\d+\"\n",
    "    content = re.sub(pattern, \"\", content)\n",
    "\n",
    "    # Remove duplicate spaces from the content\n",
    "    pattern = r\"\\s+\"\n",
    "    content = re.sub(pattern, r\" \", content)\n",
    "\n",
    "    # Add a space after punctuation marks if it doesn't exist\n",
    "    pattern = r\"([\\.!?])(\\w)\"\n",
    "    content = re.sub(pattern, r\"\\1 \\2\", content)\n",
    "\n",
    "    return content\n",
    "\n",
    "# Method for converting a docx file to a clean text file\n",
    "def docx_to_clean_file(file_in: str, file_out=None, **kwargs) -> str:\n",
    "    \"\"\"Save clean content of a docx file to text file\n",
    "\n",
    "    Args:\n",
    "        file_in (str): path to docx file\n",
    "        file_out (None, optional): path to text file\n",
    "        **kwargs (optional): arguments for other parameters\n",
    "            - no_message (bool): do not show message of result.\n",
    "                                 Default is False\n",
    "\n",
    "    Returns:\n",
    "        str: path to text file\n",
    "    \"\"\"\n",
    "    # Set default values for parameters\n",
    "    no_message = kwargs.get(\"no_message\", False)\n",
    "    if not file_out:\n",
    "        filename = splitext(file_in)[0]\n",
    "        file_out = \"%s.txt\" % filename\n",
    "        i = 0\n",
    "        while exists(file_out):\n",
    "            i += 1\n",
    "            file_out = \"%s_%s.txt\" % (filename, i)\n",
    "\n",
    "    # Clean up the content of the docx file\n",
    "    content = clean_docx(file_in)\n",
    "\n",
    "    # Write the cleaned content to a text file\n",
    "    with open(file_out, \"w+\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(content)\n",
    "\n",
    "    # Print a message if required\n",
    "    if not no_message:\n",
    "        print(\"clean content is written to file: %s\" % file_out)\n",
    "\n",
    "    return file_out\n",
    "\n",
    "# Method for counting the number of tokens in a file\n",
    "def count_tokens(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    tokens = word_tokenize(text)\n",
    "    num_tokens = len(tokens)\n",
    "    return num_tokens\n",
    "\n",
    "# Method for breaking up a file into chunks\n",
    "def break_up_file(tokens, chunk_size, overlap_size):\n",
    "    if len(tokens) <= chunk_size:\n",
    "        yield tokens\n",
    "    else:\n",
    "        chunk = tokens[:chunk_size]\n",
    "        yield chunk\n",
    "        yield from break_up_file(tokens[chunk_size-overlap_size:], chunk_size, overlap_size)\n",
    "        \n",
    "def break_up_file_to_chunks(filename, chunk_size=2000, overlap_size=100):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    return list(break_up_file(tokens, chunk_size, overlap_size))\n",
    "\n",
    "def check_overlap(chunks):\n",
    "    \"\"\"\n",
    "    Checks if the first and second chunks overlap.\n",
    "\n",
    "    Args:\n",
    "        chunks: A list of chunks of text.\n",
    "\n",
    "    Returns:\n",
    "        A string indicating if the overlap is good or not.\n",
    "    \"\"\"\n",
    "    if chunks[0][-100:] == chunks[1][:100]:\n",
    "        return 'Overlap is Good'\n",
    "    else:\n",
    "        return 'Overlap is Not Good'\n",
    "    \n",
    "\n",
    "    #converting each chunk to prompt text for openai\n",
    "def convert_to_prompt_text(tokenized_text):\n",
    "    prompt_text = \" \".join(tokenized_text)\n",
    "    prompt_text = prompt_text.replace(\" 's\", \"'s\")\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "def generate_mom (meeting_details):\n",
    "    # Extract meeting information\n",
    "    meeting_info = {}\n",
    "    for line in meeting_details.split(\"\\n\"):\n",
    "        if \":\" in line:\n",
    "            key, value = line.split(\": \")\n",
    "            meeting_info[key] = value\n",
    "\n",
    "    # Extract the date from the filename\n",
    "    filename = os.path.basename(filepath)\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', filename)\n",
    "    date = match.group() if match else \"No date found in filename.\"\n",
    "\n",
    "    # Convert the action items to a list\n",
    "    #action_items = [item.strip() for item in action_items.split(\"\\n\") if item.strip()]\n",
    "\n",
    "    # Create a new Word document\n",
    "    doc = docx.Document()\n",
    "\n",
    "    # Add the meeting information to the document\n",
    "    doc.add_paragraph(f\"Program name: {meeting_info['Program Name']}\")\n",
    "    doc.add_paragraph(f\"Chair: {meeting_info['Chair']}\")\n",
    "    doc.add_paragraph(f\"Date: {date}\")\n",
    "    doc.add_paragraph(f\"Attendees: {meeting_info['Attendees']}\")\n",
    "\n",
    "    # Add the meeting summary to the document\n",
    "    doc.add_paragraph(\"Meeting Summary:\")\n",
    "    doc.add_paragraph(meeting_summary)\n",
    "\n",
    "    # Add the items discussed to the document\n",
    "    # doc.add_paragraph(\"Items Discussed:\")\n",
    "    for item in items_discussed.split(\"\\n\"):\n",
    "        doc.add_paragraph(item.strip())\n",
    "\n",
    "    # Add the action items to the document\n",
    "    # doc.add_paragraph(\"Action Items:\")\n",
    "    for item in action_items.split(\"\\n\"):\n",
    "        doc.add_paragraph(item)\n",
    "\n",
    "    # Save the document\n",
    "    doc.save(\"./items_discussed.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f4dcd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean content is written to file: ././FW_ Important_ PAC Minutes/Touchbase - Placement (MS Teams)_2023-03-20_2.txt\n",
      "Number of tokens: 2542\n",
      "Chunk 0: 2000 tokens\n",
      "Chunk 1: 642 tokens\n",
      "Overlap is Good\n"
     ]
    }
   ],
   "source": [
    "# Set the file path\n",
    "filepath = \"././FW_ Important_ PAC Minutes/Touchbase - Placement (MS Teams)_2023-03-20.docx\"\n",
    "\n",
    "# Get the cleaned text file by calling the docx_to_clean_file function\n",
    "# It reads the content of the docx file, cleans it, and saves the cleaned content to a text file.\n",
    "# Then returns the path of the text file.\n",
    "cleaned_file = docx_to_clean_file(filepath)\n",
    "\n",
    "# Count the tokens of the cleaned file by calling the count_tokens function\n",
    "# It tokenizes the text file and returns the number of tokens in the file.\n",
    "token_count = count_tokens(cleaned_file)\n",
    "print(f\"Number of tokens: {token_count}\")\n",
    "\n",
    "# Break the cleaned file into chunks of text by calling the break_up_file_to_chunks function.\n",
    "# It reads the cleaned text file, tokenizes it, and breaks it into chunks of specified size.\n",
    "chunks = break_up_file_to_chunks(cleaned_file)\n",
    "\n",
    "# Print the length of each chunk in tokens.\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(chunk)} tokens\")\n",
    "\n",
    "# Check if the overlap is good or not by calling the check_overlap function.\n",
    "# It checks if the last 100 tokens of the first chunk and the first 100 tokens of the second chunk are the same.\n",
    "# If they are the same, it returns \"Overlap is Good\" else \"Overlap is Not Good\".\n",
    "print(check_overlap(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc593814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' I think you can finish all of them .\\n\\nPatrick Egbunonu discussed various free and paid training programs available for the students. They include Power BI, a chatbot course, a LinkedIn learning program, and a course on Excel macros and VBA. The Power BI training will take the students through the steps to build a dashboard, while the chatbot course will teach them how to build a chatbot using IBM Watson and they will get a certificate at the end of the course. The LinkedIn learning program is around AI feature of AI and ChatGPT 3 and 4. The Excel macros and VBA course will cost $54 and will take around 15-20 hours to complete. Patrick Egbunonu recommended that the students put in 40-50 hours this week to finish all the trainings.', '\\n\\nAt this meeting, Patrick Egbunonu discussed a training that will take 40 hours to complete. He specified that the first part will take 10 hours and the second part will take 5-7 hours. The meeting ended with Patrick reminding everyone to reach out for help if they are stuck and to meet in person the following week.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import openai\n",
    "import docx\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "prompt_response = []\n",
    "# Break up the file into chunks of text\n",
    "chunks = break_up_file_to_chunks(cleaned_file)\n",
    "\n",
    "# Iterate over each chunk and generate a prompt to summarize it using OpenAI's API\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Convert the chunk to prompt text\n",
    "    prompt_request = \"Summarize this meeting transcript: \" + convert_to_prompt_text(chunks[i])\n",
    "\n",
    "    # Use OpenAI's API to summarize the text\n",
    "    response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=prompt_request,\n",
    "            temperature=.5,\n",
    "            max_tokens=500,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    # Add the generated summary to the list of prompt responses\n",
    "    prompt_response.append(response[\"choices\"][0][\"text\"])\n",
    "\n",
    "# Print the generated prompt responses\n",
    "print(prompt_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cedd44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "At the meeting, Patrick Egbunonu presented various free and paid training programs available for the students. These included Power BI, a chatbot course, a LinkedIn learning program, and a course on Excel macros and VBA. Patrick Egbunonu recommended that the students put in 40-50 hours this week to finish all the trainings. He also specified that the first part will take 10 hours and the second part will take 5-7 hours. The meeting concluded with Patrick reminding everyone to reach out for help if they are stuck and to meet in person the following week.\n",
      "\n",
      "\n",
      "Main Items Discussed: \n",
      "1. Various free and paid training programs available for the students, including Power BI, a chatbot course, a LinkedIn learning program, and a course on Excel macros and VBA. \n",
      "2. Total training time of 40 hours, with the first part taking 10 hours and the second part taking 5-7 hours. \n",
      "3. Reminder to reach out for help if stuck and to meet in person the following week.\n",
      "\n",
      "\n",
      "Action Items/Recommendations:\n",
      "1. Have students complete the Power BI training to build a dashboard.\n",
      "2. Have students complete the chatbot course to learn how to build a chatbot using IBM Watson and receive a certificate.\n",
      "3. Have students complete the LinkedIn learning program around AI features of AI and ChatGPT 3 and 4.\n",
      "4. Have students complete the Excel macros and VBA course for $54.\n",
      "5. Have students put in 40-50 hours this week to finish all the trainings.\n",
      "6. Have students complete the first part of the training in 10 hours and the second part in 5-7 hours.\n",
      "7. Remind students to reach out for help if they are stuck.\n",
      "8. Meet in person the following week to discuss progress.\n",
      "\n",
      "\n",
      "Program Name: Power BI, chatbot course, LinkedIn learning program, Excel macros and VBA course\n",
      "Chair: Patrick Egbunonu \n",
      "Attendees: unspecified\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# prompt_request = \"Consoloidate these meeting summaries: \" + str(prompt_response)\n",
    "\n",
    "# response = openai.Completion.create(\n",
    "#         model=\"text-davinci-003\",\n",
    "#         prompt=prompt_request,\n",
    "#         temperature=.5,\n",
    "#         max_tokens=1000,\n",
    "#         top_p=1,\n",
    "#         frequency_penalty=0,\n",
    "#         presence_penalty=0\n",
    "#     )\n",
    "# items_discussed = response[\"choices\"][0][\"text\"]\n",
    "# print(items_discussed)  \n",
    "\n",
    "prompt_request = \"Prepare a meeting summary to put it in a minutes of meeting: \" + str(prompt_response)\n",
    "\n",
    "response_1 = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt_request,\n",
    "        temperature=.5,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "meeting_summary = response_1[\"choices\"][0][\"text\"]\n",
    "print(meeting_summary)  \n",
    "\n",
    "prompt_request = \"Prepare a list of main items discussed from the given transcript to put it in a minutes of meeting: \" + str(prompt_response)\n",
    "\n",
    "response_2 = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt_request,\n",
    "        temperature=.5,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "items_discussed = response_2[\"choices\"][0][\"text\"]\n",
    "print(items_discussed)  \n",
    "\n",
    "prompt_request = \"Provide a list of action items/recommendations from the provided meeting transcript text: \" + str(prompt_response)\n",
    "\n",
    "response_3 = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt_request,\n",
    "        temperature=.5,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "action_items = response_3[\"choices\"][0][\"text\"]\n",
    "print(action_items) \n",
    "\n",
    "prompt_request = \"Please provide Program name, Chair, Attendees from the provided meeting transcript text: \" + str(prompt_response)\n",
    "\n",
    "response_4 = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt_request,\n",
    "        temperature=.5,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "meeting_details = response_4[\"choices\"][0][\"text\"]\n",
    "print(meeting_details) \n",
    "\n",
    "# Check if all required variables are not None before calling the generate_mom function\n",
    "if (meeting_summary is not None and action_items is not None and meeting_details is not None):\n",
    "    # Call the generate_mom function with meeting_details as an argument\n",
    "    generate_mom(meeting_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8013428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
